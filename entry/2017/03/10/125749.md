
+++
date = "2017-03-10 12:57:49 +0000 UTC"
draft = false
title = "CourseraのMachine Learningから線形回帰を学んだのでまとめてみた"
tags = ["機械学習","線形回帰","Coursera"]

+++
CourseraのMachine Learningを受講しています。時間を見つけてはコツコツ進めて今のところWeek4に差し掛かったところです。Week4ではNeural Networksの話に入り一段とレベルが高くなった印象を受けています。Week1からWeek3までに学んだことを復習する必要があるなと焦りが生まれました。

受講中は配布される資料や動画を見たり他の日本人の方のブログを拝見したりと課題に取り組んできました。このタイミングで復習して整理することでWeek4以降の学習が快適になるのではないかと淡い期待を込めてWeek1からまとめてみます。

### Machine Learning - Stanford University | Coursera

こちらが受講しているMachine Learningのコースです。MOOCは好きな時間に進められるし前編英語（動画は日本語字幕あり）なので英語の学習になってオススメです。

[Machine Learning - Stanford University | Coursera](https://www.coursera.org/learn/machine-learning)

### どこまでの内容をまとめるか

このエントリではWeek1の内容に触れています。

## 線形回帰とは？

ある土地の家の価格とその家の部屋数の相関をグラフで表すと以下のようになったとします。

{{< figure src="/images/2017/03/10/125749/20170310101210.png"  >}}

Xが部屋数（RM）でYが価格（MEDV）です。

（出典：<a href="https://archive.ics.uci.edu/ml/datasets/Housing">Housing Data Set</a>）

※ このデータは部屋数以外に、その土地の犯罪率だったり児童と教師の比率など複数の要素から構成されています。

部屋数が4つの場合や部屋数が7つの場合はグラフから予想できそうです。
このとき頭の中ではグラフに右肩上がりな直線をイメージして予想できますが、この直線を数式から導き出すことを学びました。
この導き方は統計学では回帰分析の一種として<a href="https://ja.wikipedia.org/wiki/%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0">線形回帰</a>と呼ばれています。

### 仮定関数と目的関数

部屋数をX、部屋の価格をYとすると相関を表すグラフを引くための1次関数の式は次のようになります。（懐かしい数式）

<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Cdisplaystyle%0AY%20%3D%20aX%20%2B%20b%0A" alt=" \displaystyle
Y = aX + b
"/>

Machine Learningのコースではθをつかって次のような式で定義しています。

<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Cdisplaystyle%0Ah_%5Ctheta%20%28X%29%20%3D%20%5Ctheta_0%20%2B%20%5Ctheta_1%28X%29%0A" alt=" \displaystyle
h_\theta (X) = \theta_0 + \theta_1(X)
"/>

<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20h_%5Ctheta%20%28X%29" alt=" h_\theta (X)"/> は**仮定関数**と呼ばれます。家の価格予想に最適な直線を引くために<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Ctheta_0" alt=" \theta_0"/> と<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Ctheta_1" alt=" \theta_1"/>の数値を変えながらグラフにフィットした仮定関数を導き出します。
つまり右肩上がりの直線のグラフを引くために最適な<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%20aX%20%2B%20b" alt="  aX + b"/> の <code>a</code>と<code>b</code>を決めるということですね。

仮定関数がフィットしているか計算するための関数である**目的関数**があります。

<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%7B%20%5Cdisplaystyle%0AJ%28%5Ctheta_0%2C%20%5Ctheta_1%29%20%3D%20%5Cfrac%7B1%7D%7B2m%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%28h_%5Ctheta%20%28X_i%29%20-%20Y_i%29%5E2%0A%7D" alt="{ \displaystyle
J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m} (h_\theta (X_i) - Y_i)^2
}"/>

この目的関数をつかって<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20J%28%5Ctheta_0%2C%20%5Ctheta_1%29" alt=" J(\theta_0, \theta_1)"/>を最小にする<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Ctheta_0" alt=" \theta_0"/> と<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Ctheta_1" alt=" \theta_1"/>を導き出します。

### 最急降下法

目的関数を最小にする方法に**最急降下法**があります。

<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%7B%20%5Cdisplaystyle%0A%5Ctheta_j%20%3D%20%5Ctheta_j%20-%20%5Calpha%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%28h_%5Ctheta%20%28x%5E%7B%28i%29%7D%29%20-%20y%5E%7B%28i%29%7D%29%20x_j%5C%20%5E%7B%28i%29%7D%0A%7D" alt="{ \displaystyle
\theta_j = \theta_j - \alpha \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y^{(i)}) x_j\ ^{(i)}
}"/>

ここで微分が出てきます。微分は傾きを求めますがこのアルゴリズムを使い傾きが最小になるまで学習を繰り返していきます。傾きの値が最小になるほどグラフにフィットした<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%5Ctheta_j" alt=" \theta_j"/>が求められます。
<img src="http://chart.apis.google.com/chart?cht=tx&amp;chl=%20%20%5Calpha" alt="  \alpha"/>は学習率と呼ばれ数値が大きいほど傾きの変動幅が大きくなりフィットしたデータが得られず、小さいほど学習はゆっくりと進み確実にデータにフィットした値が求められます。

### Octaveでプログラム化する

コースの課題では<a href="https://ja.wikipedia.org/wiki/GNU_Octave">Octave</a>を使いプログラミング課題を提出します。
最初に示した家の価格と部屋数のグラフのデータを使い、更にこれまでのアルゴリズムからデータにフィットした直線をグラフにプロットしてみます。

**目的関数**

```
function J = computeCost(X, y, theta)
    m = length(y);
    J = sum((X*theta -y).^2) / (2* m);
end
```


**最急降下法**

```
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
    m = length(y);
    J_history = zeros(num_iters, 1);

    for iter = 1:num_iters
        theta = theta - alpha / m * X&#39; * (X * theta -y);
        J_history(iter) = computeCost(X, y, theta);
    end
end
```


**目的関数を求めグラフにプロットする。そして部屋数が5つのときの価格を予想する。**

```
%% Initialization
clear ; close all; clc
data = load("housing.txt");
x = data(:, 6);
y = data(:, 14);
m = length(y);

theta = zeros(2, 1);
X = [ones(m, 1), x]

%% Compute Cost
J = computeCost(X, y, theta)

%% Gradient Descent
iterations = 1500;
alpha = 0.01;
[theta J_history] = gradientDescent(X, y, theta, alpha, iterations);

%% Output
fprintf(&#39;Initial cost = %f\n&#39;, J);
fprintf(&#39;Final cost = %f\n&#39;, J_history(iterations));
fprintf(&#39;Theta found by gradient descent: &#39;);
fprintf(&#39;%f %f \n&#39;, theta(1), theta(2));

%% Plot data
figure; hold on;
plot(x, y, &#39;r+&#39;, &#39;LineWidth&#39;, 2);
plot(X(:,2), X*theta, &#39;-&#39;)
xlabel(&#39;RM&#39;);
ylabel(&#39;MEDV&#39;);

%% Predict
fprintf(&#39;for RM = 5, MEDV = %f\n&#39;, [1, 5] *theta);
```


**出力したグラフ**

{{< figure src="/images/2017/03/10/125749/20170310115327.png"  >}}

データにフィットした直線がプロットできたようです。

**出力した数値**

```
J =  296.07
initial cost 296.073458
final cost 27.131052
Theta found by gradient descent: -5.254087 4.477681
for RM = 5, MEDV = 17.134317
```


部屋数が５つのときに <code>17.134317</code>と予想できました。グラフにプロットされたデータとフィットされているようです。

### まとめ

<ul>
<li>最後に出したグラフですが、もう少し角度が急な直線のほうがグラフにフィットしているようです。プロットして上手く行かなければ調整して再度プロットして、の繰り返しが機械学習の大事な工程なのでしょう。</li>
<li>線形回帰はこの先で学ぶ機械学習の知識のベースになっています。データを分類する場合などに流用できます。</li>
<li>今回の学習データは<code>部屋数</code>の１つでしたが部屋数に加え<code>犯罪率</code>、<code>児童と教師の比率</code>などの複数の要素からも線形回帰をベースに家の価格を予想できる。（重回帰分析）この内容はWeek2で学びました。</li>
<li>MOOCで学び、コースを修了した先輩たちのブログで学び、自分のブログでアウトプットする、など多角的に学ぶことが大事。（ここに<code>書籍を読んで学ぶ</code>も入れたい:money_with_wings: ）</li>
<li>数式書くの大変。</li>
</ul>



